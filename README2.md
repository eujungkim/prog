# Cloud
- Cloud
  - AWS Document (https://docs.aws.amazon.com)
    - https://aws.amazon.com/ko/
  - GCP Document (https://cloud.google.com/docs/)
  - Azure Document (https://docs.microsoft.com/en-us/learn/, https://www.microsoft.com/handsonlabs)
    - https://azure.microsoft.com/ko-kr 

- AWS Document
  - Compute > EC2, EC2 Image Builder, Lambda
  - Containers > ECR, ECS, EKS, App2Container
  - Networking & Content Delivery > API Gateway, VPC, Route3
  - Storage > S3, EBS, EFS, S3 Glacier
  - Security, Identity, & Compliance > IAM
  - Management & Governance > CloudFormation, CloudWatch, AppConfig, Auto Scaling
  
- GCP Document
  - 컴퓨팅 > Compute Engine, 선점형 선점형 VM, 보안 VM,
  - 컨테이너 > GKE, Container Registry,
  - 네트워킹 > VPC, Cloud Router, Cloud NAT, Cloud Load Balancing
  - 보안 및 ID > IAM, Resource Manager, 보안 VM, VPC 서비스 제어 , Cloud Key Management Service
  - 서버리스 서버리스 컴퓨팅 컴퓨팅 > Cloud Functions, Cloud Run
  - 관리도구 관리도구 > Cloud Console, Cloud Shell, 비용 관리 , Cloud APIs
  
- Azure Document
  - Azure Fundamentals : Describe core Azure concepts
  - Azure Fundamentals : Describe core Azure services
  - Azure Fundamentals : Describe core solutions and management tools on Azure
  - Azure Fundamentals : Describe general security and network security features
  - Azure Fundamentals : Describe identity, governance, privacy, and compliance features
  - Azure Fundamentals : Describe Azure cost management and service level agreements

# AI
## 1편
- AI
  - 인공지능은 저절로 똑똑해질 수 없다. 인공지능의 90% 이상은 지도 학습(Supervised Learning) 방식으로 훈련된다.
  - 지도 학습 : 사람이 데이터 한 건 한건을 어떻게 판단하거나 처리하는지에 대한 정답을 인공지능에게 알려주면서 진행하는 방식
  - 강 인공지능 (Strong AI, General AI) : 인간의 명령 없이 스스로 판단 및 결정, 영화상의 인공지능, 범용성
  - 약 인공지능 (Weak AI, Narrow AI) : 일상생황에서 만나는 인공지능, 제한된 환경에서 구체적인 특정 업무 수행, 알파고
  - AI (Artificial Intelligence) : 사람이 해야 할 일을 기계가 대신할 수 있는 모든 자동화에 해당
  - Machine Learning : 명시적으로 규칙을 프로그래밍하지 않고 데이터로부터 의사결정을 위한 패턴을 기계가 스스로 학습
  - Deep Learning : 인공신경망 기반의 모델로, 비정형 데이터로부터 특징 추출 및 판단까지 기계가 한번에 수행
  - AI > Machine Learning > Deep Learning
  - 자사에서 말하는 AI = 딥러닝 기반의 인공지능

- 기계자동화(규칙 기반의 AI)와 머신러닝, 딥러닝 구분 기준
  - 두 가지를 사람이 하느냐, 기계에게 맡기느냐
    - 특징 추출 : 문제 해결을 위해 어떤 정보가 유용할지, 중요한 것을 꺼내는 과정
    - 판단 방식 : 모델을 만드는 단계
  - 데이터/현상 > (특징 추출) > 특징 > (판단) > 인식 및 예측
    - 규칙 기반 프로그래밍 : 사람이 특징 추출 및 판단
    - 전통적인 머신러닝 : 문제해결에 필요한 주 특징만 사람이 결정, 판단 모델은 수학적 모델링과 기계학습을 통해 구현 (회귀 분석, 의사결정나무)
    - 딥러닝 : Raw Data로부터 수학적 모델링과 인공신경망 학습을 통해 모든 것을 구현하는 방식, 이미지/텍스트 데이터를 그대로 처리 (CNN, RNN)
## 2편
- 딥러닝 : 기계가 **비정형** 데이터를 입력받은 후 데이터의 주요한 특징을 알아서 추출하고 이를 바탕으로 의사결정을 하는 기술
- 정형 데이터 (structured data)
  - 전통적인 머신러닝 기반의 기술들로 다룰 수 있는 데이터. DB, 엑셀, CSV 파일 등 사람이 정제하고 정리한 데이터
  - 머신러닝은 정형데이터를 다루는데 특화되어 있음. 빅데이터 분석 기법으로 R, SAS, SPSS 같은 통계분석 툴 활용
  - 실수 값 예측에 쓰이는 선형/로지스틱 회기분석, 카테고리 분류에 쓰이는 의사 결정 나무, 시계열 예측에 쓰이는 ARMA, ARIMA 모형 등
- 비정형 데이터 (unstructured data)
  - 딥러닝 기반의 AI가 다루는 데이터. 정리되지 않은 다양한 형식의 raw data에 가까운 데이터
  - 텍스트 데이터 예 : 웹 페이지, 상품 리뷰, SNS 글, 기업용 문서, 뉴스기사
- 인공신경망
  - 사전적 정의가 아닌 특징을 배우는 사람의 학습방식을 기계학습에 녹인 것이 딥러닝
  - 인공뉴런 : 이전의 뉴런이 넘겨준 데이터를 가중합 연산을 한 뒤, 비선형함수를 적용하여 정보를 가공하여 다음 인공 뉴런으로 데이터를 넘김
  - 딥러닝 모델 : 데이터를 통해 자동으로 필요한 특징을 찾아내고 분류를 수행한다
  - 인공 뉴런을 다양한 방식으로 여러 층으로 쌓아 연결한 것이 딥러닝의 기본 구조인 인공신경망이다.
## 3편
이미지 
- 인공신경망은 가중합와 비선형 함수로 이루어진 연산을 수행해야 하므로 입력 데이터로 벡터나 행렬 형태를 필요로 한다.
- 흑백 이미지 - 2차원 행렬(matrix), 컬러 이미지 - 3차원 텐서(tensor)
- Convolutional Neural Network (CNN)
  - 이미지 처리에 특화된 신경망
  - 이미지로부터 특징을 추출하는 **Feature Extraction** 영역과 특정 태스크 수행을 위해 덧붙이는 **태스크 수행 영역**으로 구성됨
- 특징 추출 (Feature Extraction)
  - 컨볼루션 연산과 풀링 연산 수행, Feature Learning이라고 부름
  - 컨볼루션 연산 : 컨볼루션 필터(또는 커널)가 입력 이미지를 상하좌우로 훑으면서 주요한 특징을 찾아내는 과정. 
    - 컨볼루션 필터를 여러 개 설정하면 이미지로부터 다양한 특징을 찾을 수 있음
    - 찾아낸 결과 특징을 Feature map (또는 convolved feature)라고 부른다.
    - 가중합 + 비선형 함수 적용으로 구성된다.
  - 폴링 연산
    - 컨볼루션 필터를 통해 찾은 특징으로부터 정보를 추리는 풀링 연산
    - Feature map을 상하좌우로 훑으면서 핵심적인 정보만 영역별로 샘플링
    - 주로 영역 내 가장 큰 값만을 남기고 나머지는 버리는 MaxPooling 방식을 적용
- 태스크 수행
  - 찾아낸 주요 특징 정보를 활용하여 목표로 하는 태스크 수행
  - 다음은 이미지 데이터를 처리하는 대표적인 태스크
  - Classification : 입력받은 이미지를 지정된 K개의 클래스(또는 카테고리) 중 하나로 분류 (강아지/고양이, 양호/불량)
  - Detection : 입력받은 이미지에서 특정 개체의 위치 좌표값(x,y)을 찾는다
  - Segmentation : 픽셀 단위로 영역 구분 (도로 위의 다양한 객체 영역 구별)
## 4편
언어 처리
- 자연어이해(NLU, Natual Language Understanding), 자연어처리(NLP, Natural Language Processing)
- Tokenizing(Parsing) : 문장을 세부 단위로 쪼개는 작업. 언어/태스크/데이터 특징에 따라 쪼개는 단위는 달라짐.
- 워드임베딩(word embedding) : 쪼개진 토큰을 벡터화하는 것
  - 원-핫 인코딩(One-hot Encoding) : 간단하지만 토큰수만큼의 길이를 갖는 벡터가 필요, 토큰 간의 연관관계 표현 불가, 모든 토큰이 서로 대등하며 동일한 양의 정보를 가져야 하므로 0과 1로만 표현
  - CBOW : 인공지능에게 문장을 알려주되 중간중간 빈칸을 만들어 들어갈 단어를 유추시킨다.
  - Skip-gram : 인공지능에게 단어(토큰) 하나를 알려주고, 주변에 등장할 그럴싸한 문맥을 만들도록 시킨다.
  - <img src="https://user-images.githubusercontent.com/7552395/236666234-589b2a41-e7af-49e6-87b4-f78c2ec1a542.png" width="50%" height="50%"></img>
  - CBOW, Skip-gram은 많은 단어를 학습시킬수록 더 좋은 품질의 벡터가 나온다.
  - 유사한 의미를 갖는 토큰은 유사한 벡터값을 가지도록 학습된다. 토큰끼리의 의미 연산(벡터 연산)이 가능하다.
  - 원-핫 인코딩에 비해 벡터의 길이가 훨씬 작아 저장공간을 효율적으로 사용한다.
- 대표적인 자연어이해 태스크
  - 문장/문서 분류(Sentence/Document Classification)
    - 입력받은 텍스트를 지정된 K개의 클래스(카테고리) 중 하나로 분류
    - 사용자 리뷰에 대한 감성분석(긍정/부정), 유저 발화문를 챗봇이 처리할 수 있는 기능 중 하나로 매핑하는 의도분류
  - Sequence-to-Sequence
    - 문장/문서를 입력받아 문장을 출력 : 번역, 요약, 자유대화
  - 질의 응답(Question Answering)
    - MRC(Machine Reading Comprehension) : 질문에 대해 매뉴얼 내에서 가장 답변이 될 가능성이 높은 영역 리턴
    - IR(Information Retrieval) : 질문에 대해 가장 유사한 과거 질문/답변(F&Q)를 꺼내주는 형태
## 5편
시계열 데이터 처리
- Recurrent Neural Network (RNN, 순환 신경망)
  - <img src="https://user-images.githubusercontent.com/7552395/236670818-44e6acbd-4f78-4dc3-9cd5-5551827acaa7.png" width="50%" height="50%"></img>
  - 기존 인공신경망 : 입력값으로 출력값을 예측
  - RNN : 과거의 처리 이력을 압축하여 반영
- RNN 장점
  - 시간 흐름에 따른 과거 정보를 누적할 수 있다.
  - 가변 길이의 데이터를 처리할 수 있다. (시간 단위를 구성하기 나름)
  - 다양한 구성의 모델을 만들 수 있다. 구조가 유연. (one to one, one to many, many to one, many to many)
  - 인코딩-입력 데이터 정보를 누적하는 부분, 디코딩-결과를 출력하는 부분
- RNN 단점
  - 연산 속도가 느리다
    - 현 시점의 데이터를 처리하려면 이전 시점의 데이터 처리가 완료되어야 함. 순처처리므로 GPU 칩의 병렬처리 이점을 잘 활용할 수 없음.
    - 정형 데이터(수치, 범주형) 활용시 속도 저하를 체감하지 못하며, 텍스트 데이터 처리시 연산 속도 저하 문제 발생
  - 학습이 불안정하다
    - 다루는 데이터의 timestep이 길수록 문제 발생 확률 높음
    - Gradient Exploding : timestep이 길어지면 인공신경망이 학습해야 할 값이 폭발적으로 증가하는 현상
    - Gradient Vanishing : timestep이 길어지면 오래된 과거 이력이 현재 추론에 거의 영향을 미치지 못하는 문제
  - 과거 정보를 잘 활용할 수 있는 모델이 아니다
    - 장기 종속성/의존성 문제(Long-term dependency)
    - 한 timestep씩 정보를 누적하여 인코딩하므로, 먼 과거 정보는 여러 번 압축되고 누적되어 거의 영향을 미치지 못함
- RNN 성능 보완
  - LSTM(Long-short term memory)
    - 과거 정보 중 중요한 것은 기억하고, 불필요한 것은 잊어버리도록 스스로 조절 가능한 RNN 유닛
    - ![image](https://github.com/eujungkim/prog/assets/7552395/ad2d2ba1-99c3-4fb4-b147-00f72050a221)
    - A : forget gate(불필요한 과거 정보 잊기)
    - B : input gate(현재 정보를 얼마나 반영할지 결정)
    - C : output gate(현재 시점에 연산된 최종 정보를 다음 시점에 얼마나 넘길지 결정)
    - GRU(Gated Recurrent Unit)도 유사. 기본 RNN은 거의 사용하지 않고 LSTM, GRU 사용
- 활용 사례 : 태양광 에너지 발전량 예측, 텍스트 문장 번역
## 6편
오버피팅(Overfitting), 정규화(Regularization)
- AI Process
- <img src="https://user-images.githubusercontent.com/7552395/236672821-56941e35-be75-4331-ac3e-00e402764f4c.png" width="50%" height="50%"></img>
  - Offline Process (Training Pipeline)
    - Historical data : 과거에 이미 만들어진 데이터, DB 등에 이미 수집된 데이터 
    - Generate Feature : 기 확보된 데이터를 정제하여 필요한 부분을 취한다
    - Collect labels : 라벨(AI 학습을 위해 필요한 정보, 인공지능이 맞춰야 하는 정답)을 붙인다
    - Validate & Select models : 성능 목표를 달성할 때까지
    - Train models : 반복 실험 진행
    - 튜닝 : 실험(Experimentation) 반복, 모델 학습을 위한 여러 설정 수치값(하이퍼파라미터) 조절 등
    - Publish model : 개선된 성능의 모델을 최종 선택
  - Online Process (Inference Pipeline)
    - Load model : AI 모델을 운영환경에 적용
    - 실전 환경에서 추론(inference)
- Generalization : 일반화 성능, 이전에 본적 없는 데이터에 대해서도 잘 수행하는 능력
- Overfitting : 훈련시에만 잘 작동하고 일반화 성능이 떨어지는 모델
- Dataset : Training, Validation, Test (8:1:1, 6:2:2)
  - Training set : 머신러닝, 딥러닝 모델을 학습하는데 이용하는 데이터. 정답을 알려줄 데이터.
  - Validation set : 모델을 튜닝하는데 도움을 주는 데이터. 모델의 일반화 성능을 판단하여 이어질 시험을 계획하는데 이용
  - Test set : 학습에 관여하지 않고, 모델의 최종 성능을 평가하기 위한 데이터
- 오버피팅 : Training set에 대해서만 성능이 개선되고, Validation set에 대해서는 별다른 향상이 없는 시점이 오버피팅 발생 시점
- Regularization : 오버피팅을 피하기 위한 모든 전략
  - 데이터 증강(Data Augmentation) : 데이터 변형을 통해 데이터 건수가 많아지는 효과, 과도한 변형은 오히려 해
  - Capacity 줄이기
    - Capacity : 모델의 복잡한 정도를 나타냄. 신경망이 여러층이거나 뉴런 수가 많을수록 높아진다.
    - Capacity가 필요 이상으로 너무 높은 모델은 주어진 데이터를 외우될 가능성이 높음
    - 수행하려는 태스크에 알맞는 Capacity를 가진 모델을 선택하는 것이 중요
  - 조기 종료(Early Stopping) : 오버피팅이 감지될 경우 목표 학습 시간 전이라도 조기 종료 
  - 드롭아웃(Dropout) : 일정 비율의 노드(인공 뉴런)을 무작위로 끄고 진행하는 기법
## 7편
전이학습(Transfer Learning)
- 쉽지 않은 인공지능 적용
  - 구체적이지 않으며 불명확한 태스크
  - 적은 데이터, 낮은 품질의 데이터
  - 다른 도메인 환경 (도메인 적응 문제, domain adaptation problem)
- Transfer Learning
  - 딥러닝 모델을 재활용하는 기법, 비슷한 태스크를 다른 도메인에 적용할 때, 그 태스크를 위한 학습 데이터가 부족할 경우 유용함
  - Catastrophic forgetting : 딥러닝 모델이 새로운 정보를 학습할 때 이전에 배웠던 정보를 완전히 잊어버리는 경향. 전이(Transfer)가 필요 이상으로 과하게 발생했을 때. 아래 기법을 활용하여 방지
  - 일반적인 딥러닝 모델 : 두 층 이상의 인공신경망으로 구성. 신경망 초반에는 데이터의 구체적이고 기본적인 특징을, 후반부로 갈수록 특정 태스크를 위한 추상적이고 개념적인 특징을 학습
  - Transfer Learning을 할 때는 기본 특징 학습은 건너뛰고 바로 태스크를 위한 학습 수행
  - 레이어 동결(Layer freezing) : 후반부의 신경망 층에 대해서만 파라미터를 학습하고, 전반부의 파라미터는 학습되지 않도록 고정해놓는 기법
  - Gradual Unfreezing : 파라미터를 다 동결시켜놓았다가 학습이 진행될수록 후반부부터 서서히 동결을 푸는 방법
  - Discriminative fine-tuning: 층마다 학습률(Learning rate)의 차별을 두는 방법
    - Learning rate : 한번에 인공신경망의 파라미터를 얼마만큼 업데이트 시킬지의 정도. 사람이 설정하는 값(hyperparameter)
    - 전반부의 Learning rate는 작게, 후반부의 Learning rate는 크게 설정
- Transfer Learning 모델 이용
  - 컴퓨터 비전 : 이미지를 입력 데이터로 처리하는 경우 이미지넷(120만장, 1000개 카테고리) 데이터로 학습된 모델에 Transfer Learning을 적용하여 좋은 성능을 낼 수 있음
  - 자연어 이해(NLU) : KorQuAD (자사, 한국어 표준 질의응답 데이터)
## 8편
AI 사전학습 및 자기주도학습 (Pre-Training, Self-supervised Learning)
- 사전학습 모델
  - 전이 학습을 염두에 두고 다방면으로 활용할 수 있는 모델을 미리 만들어놓는 것
  - 여러 타스크에 활용하기 위해 여러 지식을 두루두루 학습해놓은 인공지능
  - 특정 데이터타입에 대한 일반적인 지식을 두루 배워놓는 것을 목표로 함
    - 텍스트 사전학습 모델 : 언어의 일반적인 의미와 구조
    - 이미지 사전학습 모델 : 이미지의 일반적인 특징, 색채, 형태
- 대규모 데이터에 대한 Pre-training (대규모의 오픈도메인 데이터에 대해 수행)
  - 시각 데이터에 대한 사전학습 : 이미지넷, Youtube-8M(동영상)
  - 언어 데이터에 대한 사전학습 : 무난하게 활용하기 좋은 데이터는 위키피디아, 나무위키, 세종말뭉치(국립국어원)
- Self-Supervised Learning(자가지도학습)
  - 데이터는 많은데 라벨(인공지능이 입력 데이터를 제공받으면 추론해야 하는 결과)이 없는 경우
  - 인공지능 모델 학습 방법
    - Supervised Learning(지도학습) : 정답이 달린 데이터로 모델 학습
    - Semi-Supervised Learning(반지도학습) : 정답이 있는 데이터, 없는 데이터를 섞어서 학습
    - Unsupervised Learning(비지도학습) : 정답이 달리지 않은 데이터로 모델 학습
  - Self-Supervised Learning : 사람이 만들어주는 정답 라벨이 없어도 기계가 시스템적으로 자체 라벨을 만들어 사용하는 학습 방법
    - Self-Supervised Learning 후 Transfer Learning을 하면 일반적으로 좋은 성능
  - Google BERT(Bidirectional Encoder Representations from Transformers)
    - Self-Supervised Learning 기법으로 사전학습을 하고 다양한 태스크에 Transfer Learning을 할 수 있는 대표적인 예
    - 자연어처리 연구 패러다임을 전환한 계기가 된 모델
    - '언어'라는 분야 전반에 걸쳐 지식을 두루 쌓은 '하나의 거대한 뇌'를 사전학습으로 만든다는 개념
    - 사전학습에서 상당한 양의 데이터(텍스트 코퍼스)를 커다란 모델로 학습시켰으며, 후속 태스크를 위한 Transfer Learning은 간략하게만 진행해도 좋은 성능을 낼 수 있었음
## 9편
사람의 라벨링 작업을 최대한 효율적으로 할 수 있는 방법, Active Learning(능동 학습)
- Active Learning
  - 데이터는 많으나 인공지능을 '학습시킬 데이터'를 마련하기 쉽지 않을 때 이용할 수 있는 기술
  - 라벨링을 할 수 있는 인적 자원은 있지만, 많은 수의 라벨링을 수행할 수 없을 때 효과적으로 라벨링을 하기 위한 기법
  - 라벨링을 위한 예산이 한정되었을 때, 모델의 성능을 극대화할 수 있는 라벨링 대상 데이터 찾기
  - AI 모델 학습을 시작하는 초기 개발 단계에 매우 효과적이다
- Active Learning 4단계 (목표 성능이 나올 때까지 아래 방법을 반복)
  - Training a Model : 초기 학습 데이터(labeled data)를 이용해 모델을 학습
  - Select Query : 라벨이 되지 않은 데이터 풀로부터 모델에게 도움이 되는 데이터를 선별
  - Human Labeling : 선별한 데이터를 사람이 확인하여 라벨을 태깅
  - 선별한 라벨 데이터를 기존 학습 데이터와 병합한 후, 다시 모델을 학습
- Query Strategy
  - Active Learning의 핵심은 성능 향상에 효과적인 데이터를 선별하는 방법(쿼리 전략, Query Strategy)이다.
    - 학습된 모델의 판정 값을 기반으로 뽑는 Uncertainty Sampling (판정값이 낮은 불확실한 데이터)
    - 여러 개의 모델을 동시에 학습시키면서 많은 모델이 틀리는 데이터를 선별하는 Query by committee
    - 데이터가 학습 데이터로 추가될 때, 학습된 모델이 가장 많이 변화하는 데이터를 선별하는 Expected Impact
    - 데이터가 밀집된 지역의 데이터들을 선별하는 Density weighted method
    - 데이터들을 최대한 고르게 뽑아서 전체 분포를 대표할 수 있도록 데이터를 선별하는 Core-set approach
  - Uncertainty Sampling
    - AI 모델은 가장 불확실하다(least certain)고 생각하는 데이터를 추출하여 라벨링이 필요하다고 요청
  - Query by committee
    - 여러 AI 모델간의 의견불일치를 종합 고려하는 방식
    - 여러 모델간 추론한 결과 불일치가 많은 데이터일수록 가장 헷갈리는 데이터, 즉 라벨링을 진행할 대상이 되는 것
## 10편
어텐션 메커니즘(Attention mechanism)
- 어텐션 메커니즘
  - 인공신경망이 입력 데이터의 전체 또는 일부를 되짚어 살펴보면서 어떤 부분이 의사결정에 중요한지, 중요한 부분에 집중하는 방식이다.
  - 중요한 단어에 좀 더 집중하여 전체 입력을 다시 재조정하여 입력 데이터 인코딩 벡터를 만든다.
- 어텐션 스코어(Attention score)
  - 중요한 단어에 집중한다 = 어텐션 스코어를 계산한다
  - 인공신경망 모델이 각 인코딩 timestep마다 계산된 특징(feature)를 가지고 자동으로 계산하는 0~1 사이의 값
  - 집중해야 하는 경우 1에 가까운 값
- 컨텍스트 벡터(Context vector)
  - 어텐션 스코어를 구한 후 현재 디코딩할 단어와의 관련성을 반영하여 다시 입력 문장을 인코딩함
  - 중요도에 따라 전체 문맥의 정보를 잘 반영하여 있다고 하여 컨텍스트 벡터(Context vector)라고 부른다.
  - 어텐션 메커니즘이 매번 디코딩마다 직전 단계의 벡터 뿐 아니라 과거의 모든 데이터의 특징(feature)를 고려한다.
  - 딥러닝 모델이 스스로 집중할 영역을 파악한다.
- XAI(설명가능한 인공지능, eXplainable AI)로서의 어텐션
  - 기계가 판단시 중요하게 생각하는 부분을 사람에게 알려주는 역할
  - 해당 결정을 내릴 때 어떤 부분에 집중해서 판단했는지 시각화해 보여줄 수 있다.
- Transformer
  - 트랜스포머 인공신경망은 입력 데이터끼리의 self-attention을 통해 상호 정보교환을 수행하는 것이 특징이다.
  - 문장 내의 단어들이 서로서로 정보를 파악하며, 단어 간의 관계와 문맥을 더 잘 파악할 수 있게 된다.
  - 트랜스포머는 인공신경망 발전(특히 자연어 이해)에 큰 획을 긋고 있다.
  - 순차적 계산이 필요없으므로 RNN보다 빠르며 맥락 파악을 잘 하고 CNN처럼 일부씩 보지 않고 전 영역을 아우른다.
  - 고사양의 하드웨어 스펙 요구
  - <img src="https://github.com/eujungkim/prog/assets/7552395/fea00a69-1409-4d9d-830a-29fedcbd2e0c" width="30%" height="50%"></img>

## 11편
AutoML
- AI 사이언티스트 : 튜닝 - 아키텍처 변경, 하이퍼파라미터 조절
  - 인공신경망의 층 수 및 깊이, 한 층에 들어갈 인공 뉴런의 수(필터 사이즈, 필터 수)
  - 얼마나 성큼성큼 학습시킬지(learning rate), 한 번에 학습할 데이터의 수(mini-batch size)
  - 몇 번이나 반복(epoch), 어떤 최적화 기법(optimizer), 손실 함수는 어떤 것(cost function), 활성화 함수
- AutoML(Automated Machine Learning) : 자동화된 기계학습
  - '특정 태스크를 위한 모델 학습'에 한하여 사람이 주기적으로 실험에 개입하지 않아도 AI 스스로가 반복실험을 통해 성능을 개선하는 것
- AutoML의 역할 3가지
  - Feature Engineering 자동화 : AI 모델을 학습하기 위해 데이터로부터 중요한 특징(feature)을 선택하고 인코딩하는 방식 자동화 (머신러닝에서 필요하고, 딥러닝에서는 모델 내부에서 자동으로 처리하므로 여기서는 논외)
  - 하이퍼파라미터를 자동으로 탐색 : AI 모델 학습에 필요한 사람의 설정을 자동 탐색
  - 아키텍쳐 탐색 : AI 모델의 구조 자체를 더 효율적인 방향으로 찾아준다
- 딥러닝 모델 학습에 필요한 하이퍼파라미터는 다양, 사람이 결정해야 함
  - 학습률(learning rate) : 모델의 파라미터 업데이트를 얼만큼 큰 단위로 할지를 결정
  - 미니배치 사이즈(mini-batch size) : 데이터를 얼마나 쪼개어 학습할지의 단위
  - 에폭(epoch) : 데이터를 몇 번 반복 학습할지에 대한 단위
  - 모멘텀, 컨볼루션 필터 수, 스트라이드 ...
  - 딥러닝 학습 프레임워크(TensorFlow, PyTorch 등)에서는 기본적으로 잘 작동하는 설정을 디폴트로 제공
  - 기본 설정으로 학습이 잘 되지 않으면 하이퍼파라미터를 조금씩 튜닝 필요
- 하이퍼파라미터 탐색 자동화
  - 기존 하이퍼파라미터 조합을 찾는 시도 : 그리드 서치(Grid search), 랜덤 서치(Random search)
  - 그리드 서치
    - 최적화할 하이퍼파라미터의 값 구간을 일정 단위로 나눈 후, 각 단위 조합을 테스트하여 가장 높은 성능을 낸 하이퍼파라미터 조합을 선택하는 방식
    - 단순하지만 최적화 대상이 되는 하이퍼파라미터가 많다면 경우의 수가 기하급수적으로 많아져서 탐색에 오랜 시간이 걸린다
  - 랜덤 서치
    - 랜덤하게 하이퍼파라미터의 조합을 테스트하는 방식인데, 그리드서치에 비해 비교적 빠르게 최적의 조합을 찾아내곤 한다
  - 최근의 AutoML 방식에서는 하이퍼파라미터도 모델을 통해 탐색
    - Meta Learner model : Learner model이 학습되는 것을 도와줌 (메타학습, Learn to Learn)
    - Learner model : 성능 개선을 위한 피드백 전달
    - Meta Learner는 대부분 RNN과 강화학습을 활용하여 최적의 하이퍼파라미터를 탐색
- 아키텍처 탐색 자동화
  - 아키텍처 : 모델을 이루는 구조
  - 딥러닝의 경우 인공신경망을 활용하므로, NAS(Neural Architecture search)라고 부른다
  - NAS도 Meta Learner(어떤 구조의 신경망을 만들지)와 Learner(본 과제를 수행하는 AI 모델)로 이루어짐
  - Meta Learner는 RNN과 강화학습을 접목한 형식으로 구성
  - Meta Learner는 Learner의 인공신경망 아키텍처의 구성을 결정하며, Learner의 태스크 수행 결과를 보상으로 활용한다
  - 진화 알고리즘이나 경사하강법을 기반으로 한 NAS 방식도 있다
- AutoML
  - 일반적으로 사람이 고민한 모델 이상의 성능
  - 랜덤 그래프 생성 방법론을 기반으로 NAS를 진행하자 기존 구성 방법의 틀을 완전히 깬 모델이 만들어짐
  - 시간이 오래 걸릴 수 있다
  - Learner 모델과 Meta Learner 모델이 동시다발적으로 학습해야 하므로 고사양의 하드웨어 스펙 필요
- AutoML 서비스
  - CSP 3사 모두 AutoML 서비스 제공
  - Google AutoML
    - 이미지에 대한 분류(classification)와 객체 탐지(detection)
    - 동영상에 대한 분류(classification)와 객체추적(visual tracking)
    - 자연어에 대한 분류(classification), 객체명인식(named entity recognition)
    - 감정분류(sentiment classification), 번역(translation)
    - 정형 테이블 데이터에 대한 회귀(regression) 및 분류(classification)
    - 엣지 레벨(엣지 기기 탑재)와 클라우드 레벨(API 형식) 제공
## 12편
XAI
- 룰 기반 모델, 머신러닝 기반 모델, 회귀 모형, 의사 결정 나무 등은 어떤 변수가 어떤 영향을 미치는지 해석 가능
- 규칙이나 몇 가지 입력 변수로 판단하기 어려운(특히 비정형 데이터에 대한) 태스크는, 딥러닝이 좋은 성능을 낼 수 있지만 설명력이 부족
- 설명 가능한 인공지능, XAI(eXplainable Artificial Intelligence)
  - 모델에 설명 가능한 근거와 해석력을 부여해서 투명성, 신뢰성을 확보하고자 하는 것이 목적
- DARPA(Defense Advanced Research Projects Agency) : XAI를 가장 본격적으로 연구하는 대표 기관
- 1) 기존 AI 모델에 설명할 수 있는 어떤 모듈을 덧붙이는 방식
  - 어텐션 메커니즘(Attention Mechanism)을 활용한 XAI
  - 설명하는 법 학습하기(Learn to explain) : 딥러닝 모델에 RNN 모듈 등을 덧붙여 인간이 이해할 수 있는 방식으로 설명을 생성하도록 하는 방식
  - 딥러닝 모델이 해석 가능한 모듈 구성요소로 이루어진 경우, 판정 결과가 어떤 모듈 경로를 따라 연산되는지 파악하는 모듈러 네트워크(Modular Networks) 방식
  - 딥러닝 모델에서 "설명가능"한 특징을 학습한 노드를 찾아서 그 특징(Feature)에 "설명 라벨"을 붙이는 Feature Identification 방법
- 2) 애초에 설명력있는 모델을 만드는 방법 : 의사결정나무나 선형회귀분석 모델 (여기서는 논외)
- 3) 인공신경망처럼 복잡한 블랙박스 모델의 일부분을 설명해 줄 수 있는 다른 모델을 활용하여 유추하는 방식
  - 일부 영역의 데이터(+와 O)를 활용해 설명력이 좋은 모델을 별도로 하나 더 만들어 학습시킨다
  - LIME, SP-LIME

# Library
- Http Server : Jetty 9 Embedded (https://www.eclipse.org/jetty/documentation/jetty-9/index.html#jetty-helloworld)
- Http Client : Jetty 9 HttpClient (https://www.eclipse.org/jetty/documentation/jetty-9/index.html#http-client)
- Json : Google Gson 2.8.6 (https://github.com/google/gson)
